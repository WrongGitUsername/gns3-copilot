#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLMæ¨¡å‹ç®¡ç†å™¨
è´Ÿè´£ä¸åŒLLMæ¨¡å‹çš„åˆå§‹åŒ–å’Œåˆ‡æ¢
"""

import os
from typing import Union
from langchain_ollama import OllamaLLM
from langchain_openai import ChatOpenAI
from langchain_core.language_models.base import BaseLanguageModel

# å°è¯•å¯¼å…¥ langchain-deepseek
try:
    from langchain_deepseek import ChatDeepSeek
    DEEPSEEK_AVAILABLE = True
except ImportError:
    DEEPSEEK_AVAILABLE = False
    print("âš ï¸ langchain-deepseek not installed, DeepSeek functionality unavailable")
    print("Install command: pip install langchain-deepseek")

# å¯¼å…¥è¯­è¨€é€‚é…å™¨
try:
    from .language_adapter import get_message, language_adapter
    LANGUAGE_ADAPTER_AVAILABLE = True
except ImportError:
    LANGUAGE_ADAPTER_AVAILABLE = False


class LLMManager:
    """LLMæ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self):
        self.current_model = None
        self.model_name = None
        self._deepseek_model_name = None  # ç”¨äºä¿å­˜ DeepSeek æ¨¡å‹åç§°
        # è‡ªåŠ¨åˆå§‹åŒ– LLM
        self.init_llm()
    
    def get_current_model_info(self) -> str:
        """è·å–å½“å‰æ¨¡å‹ä¿¡æ¯"""
        if not self.current_model:
            if LANGUAGE_ADAPTER_AVAILABLE:
                return get_message("model_not_initialized")
            else:
                return "Model not initialized"
        
        if isinstance(self.current_model, OllamaLLM):
            return f"Ollama: {self.current_model.model}"
        elif isinstance(self.current_model, ChatOpenAI):
            # åŒºåˆ†ä¸åŒçš„ OpenAI å…¼å®¹æœåŠ¡
            base_url = getattr(self.current_model, 'openai_api_base', None) or getattr(self.current_model, 'base_url', None)
            if base_url and 'openrouter.ai' in str(base_url):
                return f"OpenRouter: {self.current_model.model_name}"
            elif base_url:
                return f"OpenAIå…¼å®¹: {self.current_model.model_name}"
            else:
                return f"OpenAI: {self.current_model.model_name}"
        elif DEEPSEEK_AVAILABLE and isinstance(self.current_model, ChatDeepSeek):
            # ä½¿ç”¨ä¿å­˜çš„æ¨¡å‹åç§°
            model_name = self._deepseek_model_name or "deepseek-chat"
            return f"DeepSeek: {model_name}"
        else:
            return f"æœªçŸ¥æ¨¡å‹ç±»å‹: {type(self.current_model).__name__}"
    
    def init_llm(self) -> BaseLanguageModel:
        """æ™ºèƒ½åˆå§‹åŒ–LLMï¼ˆæ ¹æ®é»˜è®¤é…ç½®æˆ–ä¼˜å…ˆçº§å°è¯•ï¼‰"""
        
        # ä»ç¯å¢ƒå˜é‡è¯»å–é»˜è®¤æ¨¡å‹
        default_model = os.getenv("DEFAULT_LLM_MODEL", "").lower()
        
        # è§£æé»˜è®¤æ¨¡å‹ï¼Œæ”¯æŒç›´æ¥æŒ‡å®šæ¨¡å‹åç§°
        if "/" in default_model:  # å¦‚ "moonshotai/kimi-k2:free"
            # æ£€æŸ¥æ˜¯å¦æ˜¯ OpenRouter æ¨¡å‹
            if os.getenv("USE_OPENROUTER", "false").lower() == "true":
                if self._try_init_openrouter():
                    return self.current_model
            default_model_type = "openrouter"
        elif "deepseek" in default_model:  # æ”¯æŒ "deepseek" æˆ– "deepseek-chat"
            default_model_type = "deepseek"
        else:
            default_model_type = default_model
        
        # æ ¹æ®é»˜è®¤é…ç½®ä¼˜å…ˆå°è¯•æŒ‡å®šæ¨¡å‹
        if default_model_type == "deepseek":
            if self._try_init_deepseek():
                return self.current_model
        elif default_model_type == "openai_compatible":
            if self._try_init_openai_compatible():
                return self.current_model
        elif default_model_type == "ollama":
            if self._try_init_ollama():
                return self.current_model
        elif default_model_type == "openrouter":
            if self._try_init_openrouter():
                return self.current_model
        
        # å¦‚æœé»˜è®¤æ¨¡å‹å¤±è´¥ï¼ŒæŒ‰ä¼˜å…ˆçº§å°è¯•å…¶ä»–æ¨¡å‹
        print(f"âš ï¸ é»˜è®¤æ¨¡å‹ '{default_model}' åˆå§‹åŒ–å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ¨¡å‹...")
        
        # ä¼˜å…ˆçº§1: OpenRouter
        if os.getenv("USE_OPENROUTER", "false").lower() == "true":
            if self._try_init_openrouter():
                return self.current_model
        
        # ä¼˜å…ˆçº§2: DeepSeek
        if os.getenv("USE_DEEPSEEK", "false").lower() == "true":
            if self._try_init_deepseek():
                return self.current_model
        
        # ä¼˜å…ˆçº§3: OpenAIå…¼å®¹æ¥å£
        if os.getenv("USE_OPENAI_COMPATIBLE", "false").lower() == "true":
            if self._try_init_openai_compatible():
                return self.current_model
        
        # ä¼˜å…ˆçº§4: Ollamaæœ¬åœ°æ¨¡å‹ï¼ˆæœ€åé€‰æ‹©ï¼‰
        if self._try_init_ollama():
            return self.current_model
        
        raise Exception("âŒ æ— æ³•åˆå§‹åŒ–ä»»ä½•LLMæ¨¡å‹")
    
    def _try_init_openrouter(self) -> bool:
        """å°è¯•åˆå§‹åŒ– OpenRouter æ¨¡å‹"""
        try:
            llm = self._init_openrouter_llm()
            self.current_model = llm
            self.model_name = "OpenRouter"
            return True
        except Exception as e:
            print(f"âš ï¸ OpenRouter åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def _try_init_deepseek(self) -> bool:
        """å°è¯•åˆå§‹åŒ– DeepSeek æ¨¡å‹"""
        try:
            llm = self._init_deepseek_llm()
            self.current_model = llm
            self.model_name = "DeepSeek"
            return True
        except Exception as e:
            print(f"âš ï¸ DeepSeek åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def _try_init_openai_compatible(self) -> bool:
        """å°è¯•åˆå§‹åŒ– OpenAI å…¼å®¹æ¥å£"""
        try:
            llm = self._init_openai_compatible_llm()
            self.current_model = llm
            self.model_name = "OpenAIå…¼å®¹"
            return True
        except Exception as e:
            print(f"âš ï¸ OpenAIå…¼å®¹æ¥å£åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def _try_init_ollama(self) -> bool:
        """å°è¯•åˆå§‹åŒ– Ollama æœ¬åœ°æ¨¡å‹"""
        try:
            llm = self._init_ollama_llm()
            self.current_model = llm
            self.model_name = "Ollama"
            return True
        except Exception as e:
            print(f"âš ï¸ Ollama åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def _init_openrouter_llm(self) -> ChatOpenAI:
        """åˆå§‹åŒ– OpenRouter æ¨¡å‹"""
        base_url = os.getenv("OPENROUTER_BASE_URL", "https://openrouter.ai/api/v1")
        api_key = os.getenv("OPENROUTER_API_KEY")
        model = os.getenv("OPENROUTER_MODEL") or os.getenv("DEFAULT_LLM_MODEL", "moonshotai/kimi-k2:free")
        
        if not api_key:
            raise ValueError("OPENROUTER_API_KEY æœªè®¾ç½®")
        
        llm = ChatOpenAI(
            base_url=base_url,
            api_key=api_key,
            model=model,
            temperature=0.1,
            max_tokens=1024,
        )
        
        # æµ‹è¯•è¿æ¥
        test_response = llm.invoke("ä½ å¥½")
        print(f"ğŸ§  OpenRouter è¿æ¥æˆåŠŸ: {model}")
        return llm
    
    def _init_deepseek_llm(self):
        """åˆå§‹åŒ– DeepSeek æ¨¡å‹ï¼ˆä½¿ç”¨ langchain-deepseekï¼‰"""
        if not DEEPSEEK_AVAILABLE:
            raise ImportError("langchain-deepseek æ¨¡å—æœªå®‰è£…")
        
        api_key = os.getenv("DEEPSEEK_API_KEY")
        if not api_key:
            raise ValueError("DEEPSEEK_API_KEY æœªè®¾ç½®")
        
        model = os.getenv("DEEPSEEK_MODEL", "deepseek-chat")
        
        try:
            llm = ChatDeepSeek(
                api_key=api_key,
                model=model,
                temperature=0.1,
                max_tokens=1024,
            )
            
            # æµ‹è¯•è¿æ¥
            test_response = llm.invoke("ä½ å¥½")
            if LANGUAGE_ADAPTER_AVAILABLE:
                print(get_message("deepseek_connected", model))
            else:
                print(f"ğŸ§  DeepSeek connected successfully: {model}")
            
            # ä¿å­˜æ¨¡å‹åç§°ä»¥å¤‡åç”¨
            self._deepseek_model_name = model
            return llm
            
        except Exception as e:
            if LANGUAGE_ADAPTER_AVAILABLE:
                print(get_message("deepseek_init_failed", str(e)))
            else:
                print(f"âŒ DeepSeek initialization failed: {e}")
            # å°è¯•ä½¿ç”¨æ›´ç®€å•çš„å‚æ•°
            try:
                llm = ChatDeepSeek(api_key=api_key)
                test_response = llm.invoke("ä½ å¥½")
                if LANGUAGE_ADAPTER_AVAILABLE:
                    print(get_message("deepseek_connected_default"))
                else:
                    print("ğŸ§  DeepSeek connected successfully (using default parameters)")
                self._deepseek_model_name = "deepseek-chat"
                return llm
            except Exception as e2:
                raise Exception(f"DeepSeek initialization completely failed: {e2}")
    
    def _init_openai_compatible_llm(self) -> ChatOpenAI:
        """åˆå§‹åŒ– OpenAI å…¼å®¹æ¥å£"""
        base_url = os.getenv("OPENAI_COMPATIBLE_BASE_URL")
        api_key = os.getenv("OPENAI_COMPATIBLE_API_KEY", "dummy")
        model = os.getenv("OPENAI_COMPATIBLE_MODEL", "gpt-3.5-turbo")
        
        if not base_url:
            raise ValueError("OPENAI_COMPATIBLE_BASE_URL æœªè®¾ç½®")
        
        llm = ChatOpenAI(
            base_url=base_url,
            api_key=api_key,
            model=model,
            temperature=0.1,
            max_tokens=1024,
        )
        
        # æµ‹è¯•è¿æ¥
        test_response = llm.invoke("ä½ å¥½")
        print(f"ğŸ§  OpenAIå…¼å®¹æ¥å£è¿æ¥æˆåŠŸ: {model}")
        return llm
    
    def _init_ollama_llm(self) -> OllamaLLM:
        """åˆå§‹åŒ– Ollama æœ¬åœ°æ¨¡å‹"""
        ollama_base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        ollama_model = os.getenv("OLLAMA_MODEL", "deepseek-r1:8b")
        
        llm = OllamaLLM(
            base_url=ollama_base_url,
            model=ollama_model,
            temperature=0.1,
            num_predict=1024,
            verbose=False
        )
        
        # æµ‹è¯•è¿æ¥
        test_response = llm.invoke("ä½ å¥½")
        print(f"ğŸ§  Ollama è¿æ¥æˆåŠŸ: {ollama_model}")
        return llm
